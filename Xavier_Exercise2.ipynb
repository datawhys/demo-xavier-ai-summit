{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Existing models - examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are Python imports. Imports are great because they allow us to easily bring code in that other people have already written\n",
    "\n",
    "In this case, we're bringing in pandas and numpy, libaries for working with tabular data and number crunching, plus the scikit-learn library (sklearn) to help us build a model.\n",
    "\n",
    "We're also bringing in the mondobrain python package along with shap and lime to help us explain the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "\n",
    "import mondobrain as mb\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll start by importing our dataset below. Unlike last time, this time we'll load it from an external datafile\n",
    "\n",
    "There's a csv file and an excel file in the github repository that contains all the code for this exercise: https://github.com/datawhys/demo-xavier-ai-summit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/datawhys/demo-xavier-ai-summit/blob/main/asthma_data.csv?raw=true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One thing we almost always have to do with the machine learning library is data cleaning.\n",
    "\n",
    "A lot of machine learning libraries don't like missing values, so we're going to treat those missing values. There are a lot of strategies for this that can be picked from. First, we have to find any column that has a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_has_nulls = np.any(df.isna(), axis=0).values\n",
    "df.columns[column_has_nulls]\n",
    "\n",
    "# Now we find all of the numeric columns, and then categorical columns\n",
    "# We can't use the same approach to treat missing values in both the categorical and numeric columns\n",
    "column_is_numeric = np.array([is_numeric_dtype(v) for v in df.dtypes])\n",
    "categorical_cols = df.columns[~column_is_numeric]; categorical_cols\n",
    "\n",
    "# Let's split those into numeric columns with missing values, and categorical columns with missing values\n",
    "numeric_cols_with_nulls = df.columns[np.all(np.array([column_has_nulls, column_is_numeric]), axis=0)]\n",
    "non_numeric_cols_with_nulls = df.columns[np.all(np.array([column_has_nulls, np.bitwise_not(column_is_numeric)]), axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are going to fill those missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll make a copy of our original dataframe:\n",
    "df_imputed_nums = df.copy()\n",
    "\n",
    "# then we'll fill all the null values in all of the numeric columns with nulls with the mean of that column\n",
    "df_imputed_nums[numeric_cols_with_nulls] = df_imputed_nums[numeric_cols_with_nulls].fillna(value=df[numeric_cols_with_nulls].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have a new dataframe, and it's got all our missing values in the numerical columns replaced with the mean of each of those columns. \n",
    "\n",
    "Note: It turns out that this dataset doesn't actually have any missing values in categorical columns. If it did, we could handle things another way, perhaps by eliminating the column, or by randomly assigning a value along the same distribution as the original, or by introducing a third class, like 'missing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at our dataframe:\n",
    "\n",
    "df_imputed_nums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need to treat the categorical vars as numbers. To do that, we'll do something called One Hot Encode them\n",
    "\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "cat_feats_encoded = ohe.fit_transform(df_imputed_nums[categorical_cols])\n",
    "df_prepared = pd.get_dummies(df_imputed_nums, columns=categorical_cols)\n",
    "df_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_dummies method creates one hot encoded variables for us, but we don't need both Asthma_1. No Asthma and Asthma_2. No Asthma. In fact, having both of them would be a big problem, because we'd just find the rule that said you found Asthma when you found not \"No Asthma\". To avoid that, we're going to delete 'Asthma_1. No Asthma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_prepared['Asthma_1. No Asthma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The convention for when we train a model is to use y for the dependent variable, and X for the independent variables\n",
    "# We're going to prepare that now\n",
    "\n",
    "y = df_prepared[['Asthma_2. Asthma']]\n",
    "X = df_prepared[[c for c in df_prepared.columns if c != 'Asthma_2. Asthma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from yesterday we said we should always test on a different set of data then we train on? We're going to use this function to do that split now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth = 10,\n",
    "                             min_samples_leaf = 2,\n",
    "                             min_samples_split = 2,\n",
    "                             n_estimators = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_preds = rfc.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, pipeline_preds)\n",
    "test_roc_auc = roc_auc_score(y_test, pipeline_preds)\n",
    "test_confusion_matrix = confusion_matrix(y_test, pipeline_preds)\n",
    "\n",
    "print(f'Accuracy Score: {test_accuracy}')\n",
    "print(f'ROC AUC Score: {test_roc_auc}')\n",
    "print(f'Confusion Matrix: \\n{test_confusion_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_1 = X_test.loc[224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.loc[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a quick function that can be used to explain the instance passed\n",
    "predict_rfc_prob = lambda x: rfc.predict_proba(x).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
    "                                                        mode = 'classification',\n",
    "                                                        feature_names = X_train.columns,\n",
    "                                                        class_names = ['Asthma', 'No_Asthma'],\n",
    "                                                        random_state=528491,\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing LIME explainability for person 1\n",
    "person_1_lime = lime_explainer.explain_instance(person_1.values,\n",
    "                                                predict_rfc_prob,\n",
    "                                                num_features = 10)\n",
    "person_1_lime.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.TreeExplainer(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_1_shap_values = shap_explainer.shap_values(person_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(shap_explainer.expected_value[1], person_1_shap_values[1], person_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = mb.MondoDataFrame(df)\n",
    "\n",
    "mb.api_key = 'bDSAWfXXEw0h0WggDoDFg1ghBp5o4Myy'\n",
    "mb.api_secret = 'umXDHYSUz1oaZJ43XIIBe6ck0XofzTxDgCqmMatt52cmQroghEDI-AMVQn4py2_n'\n",
    "\n",
    "# Select a column as your outcome column & specify a target class\n",
    "outcome = mdf[\"Asthma\"]\n",
    "\n",
    "# Check the classes of your outcome variable\n",
    "outcome.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome.target_class = \"2. Asthma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorable = mdf[[c for c in mdf.columns if c != \"Asthma\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = mb.Solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.fit(explorable, outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.rule_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check this out in our interactive dashboard!\n",
    "\n",
    "To check it out, go to https://demo.mondobrain.com, and login with:\n",
    "\n",
    "username: xavier_user\n",
    "password: xavier_ai_conf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
